{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c41b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions:\n",
    "# 1- Before executing the code, ensure that all required libraries are installed.\n",
    "# 2- Download the dataset files: images_test_rev1.zip, images_training_rev1.zip, training_solutions_rev1.zip\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "def unzip(file, destination):\n",
    "    print('Unzipping to', destination)\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination)\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "train_images_path = os.path.join(base_dir, \"images_training_rev1\")\n",
    "test_images_path = os.path.join(base_dir, \"images_test_rev1\")\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", base_dir)\n",
    "\n",
    "# Check whether specific directories exist before extraction\n",
    "print(\"Before extraction:\")\n",
    "print(\"Train Images Directory Exists:\", os.path.exists(train_images_path))\n",
    "print(\"Test Images Directory Exists:\", os.path.exists(test_images_path))\n",
    "\n",
    "# Unzip files\n",
    "unzip('C:\\\\Users\\\\dduun\\\\Downloads\\\\images_training_rev1.zip', base_dir)\n",
    "unzip('C:\\\\Users\\\\dduun\\\\Downloads\\\\images_test_rev1.zip', base_dir)\n",
    "\n",
    "# Print whether directories exist after extraction\n",
    "print(\"After extraction:\")\n",
    "print(\"Train Images Directory Exists:\", os.path.exists(train_images_path))\n",
    "print(\"Test Images Directory Exists:\", os.path.exists(test_images_path))\n",
    "#importing required libraries for data exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read the training CSV file\n",
    "train_sol = pd.read_csv(\"training_solutions_rev1.csv\")\n",
    "\n",
    "# Display the first 5 rows to understand the classes\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "train_sol.head()\n",
    "# Check the number of rows and columns in the dataframe\n",
    "num_rows, num_cols = train_sol.shape\n",
    "print(f\"\\nNumber of rows: {num_rows}, Number of columns: {num_cols}\")\n",
    "\n",
    "\n",
    "# Basic statistics of the dataframe\n",
    "print(\"\\nBasic statistics of the training solutions data:\")\n",
    "print(train_sol.describe())\n",
    "\n",
    "# Check the distribution of classes\n",
    "print(\"\\nDistribution of classes:\")\n",
    "print(train_sol['GalaxyID'].value_counts())\n",
    "\n",
    "# Check the distribution of target classes\n",
    "print(\"\\nDistribution of target classes:\")\n",
    "print(train_sol[['Class1.1', 'Class1.2', 'Class1.3']].sum())\n",
    "def append_ext(filename):\n",
    "    \"\"\" Appends `.jpg` file extension to a filename \"\"\"\n",
    "    return f\"{filename}.jpg\"\n",
    "\n",
    "train_sol[\"GalaxyID\"] = train_sol[\"GalaxyID\"].apply(append_ext)\n",
    "train_sol.head()\n",
    "# Filter the dataset based on certain conditions\n",
    "new_train_sol = train_sol[((train_sol[\"Class1.1\"] >= 0.8) | (train_sol[\"Class1.2\"] >= 0.8))].reset_index(drop=True)\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"Shape of the filtered dataset:\", new_train_sol.shape)\n",
    "\n",
    "# Display the first 5 rows of the filtered DataFrame\n",
    "new_train_sol.head()\n",
    "# Check for duplicates in the new_train_sol DataFrame\n",
    "duplicates_exist = new_train_sol.duplicated().any()\n",
    "\n",
    "# If duplicates exist, remove them\n",
    "if duplicates_exist:\n",
    "    new_train_sol.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates removed from new_train_sol DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in new_train_sol DataFrame.\")\n",
    "\n",
    "# Check for missing values in the new_train_sol DataFrame\n",
    "missing_values_exist = new_train_sol.isnull().any().any()\n",
    "\n",
    "# If missing values exist, display the count\n",
    "if missing_values_exist:\n",
    "    missing_values_count = new_train_sol.isnull().sum().sum()\n",
    "    print(f\"Missing values found in new_train_sol DataFrame. Total count: {missing_values_count}\")\n",
    "else:\n",
    "    print(\"No missing values found in new_train_sol DataFrame.\")\n",
    "# Define conditions based on probability thresholds\n",
    "conditions = [\n",
    "    new_train_sol[\"Class1.1\"] >= 0.8,\n",
    "    new_train_sol[\"Class1.2\"] >= 0.8,\n",
    "]\n",
    "\n",
    "# Define corresponding choices for each condition\n",
    "choices = [\"Elliptical\", \"Spiral\"]\n",
    "\n",
    "# Create a new column in the DataFrame that displays the results of comparisons\n",
    "# The 'Class' column will contain labels based on the conditions and choices\n",
    "new_train_sol[\"Class\"] = np.select(conditions, choices, default=\"Tie\")\n",
    "\n",
    "# Display the DataFrame after adding the new 'Class' column\n",
    "new_train_sol\n",
    "# Exploring the distribution of the new dataframe\n",
    "# Displaying the shape of subsets based on the 'Class' column\n",
    "\n",
    "# Display the shape of the subset where galaxies are labeled as 'Elliptical'\n",
    "print(\"Elliptical galaxies:\", new_train_sol[new_train_sol[\"Class\"] == \"Elliptical\"].shape)\n",
    "\n",
    "# Display the shape of the subset where galaxies are labeled as 'Spiral'\n",
    "print(\"Spiral galaxies:\", new_train_sol[new_train_sol[\"Class\"] == \"Spiral\"].shape)\n",
    "\n",
    "# Display the shape of the subset where galaxies are labeled as 'Irregular'\n",
    "#print(\"Irregular galaxies:\", new_train_sol[new_train_sol[\"Class\"] == \"Irregular\"].shape)\n",
    "# Randomly selecting samples in each category and saving them into new dataframes.\n",
    "\n",
    "# Sample 3000 random spiral galaxies from new_train_sol\n",
    "df_spiral_new = new_train_sol[new_train_sol[\"Class\"] == \"Spiral\"].sample(n=3000, random_state=42)\n",
    "print(\"This is a sample of Spiral galaxies:\")\n",
    "print(df_spiral_new.head())\n",
    "print(df_spiral_new.shape)\n",
    "\n",
    "# Sample 3000 random elliptical galaxies from new_train_sol\n",
    "df_elliptical_new = new_train_sol[new_train_sol[\"Class\"] == \"Elliptical\"].sample(n=3000, random_state=42)\n",
    "print(\"This is a sample of Elliptical galaxies:\")\n",
    "print(df_elliptical_new.head())\n",
    "print(df_elliptical_new.shape)\n",
    "\n",
    "# Sample 100 random irregular galaxies from new_train_sol\n",
    "#df_irregular_new = new_train_sol[new_train_sol[\"Class\"] == \"Irregular\"].sample(n=100, random_state=42)\n",
    "#print(\"This is a sample of Irregular galaxies:\")\n",
    "#print(df_irregular_new.head())\n",
    "#print(df_irregular_new.shape)\n",
    "# Combine the dataframes for spiral, elliptical\n",
    "df_galaxies = pd.concat([df_spiral_new, df_elliptical_new], ignore_index=True)\n",
    "\n",
    "# Select only the GalaxyID and Class columns\n",
    "df_galaxies = df_galaxies[[\"GalaxyID\", \"Class\"]]\n",
    "\n",
    "# Display the first 5 rows of the merged dataframe\n",
    "print(\"Training Dataframe:\")\n",
    "df_galaxies.head()\n",
    "# Print the shape of the final training dataframe\n",
    "print(\"Shape of the Training Dataframe:\")\n",
    "print(df_galaxies.shape)\n",
    "print(df_galaxies['Class'].dtype)\n",
    "# Import necessary libraries for image processing, model optimization, and visualization\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Assuming 'Class' column contains the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_galaxies['Class'] = label_encoder.fit_transform(df_galaxies['Class'])\n",
    "\n",
    "# Convert 'Class' column to numerical labels\n",
    "df_galaxies['Class'] = pd.Categorical(df_galaxies['Class'])\n",
    "df_galaxies['Class'] = df_galaxies['Class'].cat.codes\n",
    "\n",
    "# One-hot encode the labels\n",
    "df_galaxies['Class'] = to_categorical(df_galaxies['Class'], num_classes=2)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df_galaxies = shuffle(df_galaxies, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16  # Reduced batch size\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# Initialize the VGG16 pretrained model outside the loop\n",
    "pretrained_model = VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "# Fixed dropout rate\n",
    "dropout_rate = 0.5  # Set your desired dropout rate\n",
    "\n",
    "# Iterate over hyperparameter combinations\n",
    "for learning_rate in param_grid['learning_rate']:\n",
    "    # Construct the CNN model architecture with increased dropout and weight regularization\n",
    "    input_layer = Input(shape=(150, 150, 3))\n",
    "    x = pretrained_model(input_layer)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)  # Use the variable dropout_rate\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Creating a checkpoint to save the best model\n",
    "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor=\"val_loss\", verbose=0, save_best_only=True, mode=\"auto\")\n",
    "\n",
    "    # Specifying an early stopping to avoid overfitting\n",
    "    stopping = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=0, mode=\"auto\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the model using cross-validation\n",
    "    for fold, (train_index, valid_index) in enumerate(cv.split(df_galaxies, df_galaxies['Class'])):\n",
    "        train_data = df_galaxies.iloc[train_index]\n",
    "        valid_data = df_galaxies.iloc[valid_index]\n",
    "\n",
    "        # Load and preprocess images\n",
    "        train_images = [img_to_array(load_img(os.path.join(train_images_path, f\"{img}\"), target_size=(150, 150))) for img in train_data['GalaxyID']]\n",
    "        train_images = np.array(train_images) / 255.0  # Normalize pixel values\n",
    "\n",
    "        valid_images = [img_to_array(load_img(os.path.join(train_images_path, f\"{img}\"), target_size=(150, 150))) for img in valid_data['GalaxyID']]\n",
    "        valid_images = np.array(valid_images) / 255.0  # Normalize pixel values\n",
    "\n",
    "        print(f\"Fold {fold + 1}/{cv.get_n_splits()} - Training on {len(train_images)} samples, Validating on {len(valid_images)} samples\")\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_images, train_data['Class'], epochs=3, validation_data=(valid_images, valid_data['Class']), callbacks=[checkpoint, stopping])\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_pred = model.predict(valid_images)\n",
    "        y_true = valid_data['Class']\n",
    "        y_pred_classes = np.round(y_pred).flatten().astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "        print(f'Learning Rate: {learning_rate}, Dropout Rate: {dropout_rate}, Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
